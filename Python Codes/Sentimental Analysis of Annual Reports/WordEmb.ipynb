{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text <br>\n",
    "word_tokenize: tokenizes the text <br>\n",
    "3rd line of the function is to remove the stopwords <br>\n",
    "4th line of the function is to remove the punctuations <br>\n",
    "5th line of the function is to remove the numbers <br>\n",
    "6th line of the function is to remove the words with length less than 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "# Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not re.match(r'\\d+', token)]\n",
    "    tokens = [token for token in tokens if token.strip()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the TF-DIF matrix of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TIFDIF(text):\n",
    "    # Compute TF-IDF\n",
    "    tokens = preprocess(text)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "    # Access the computed TF-IDF scores\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    data = []\n",
    "    for col in tfidf_matrix.nonzero()[1]:\n",
    "        data.append({'Token': feature_names[col], 'TF-IDF score': tfidf_matrix[0, col]})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     45\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Smith is a software engineer from New York City. He works at XYZ Company.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m preprocessed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(preprocessed_text)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m phrase_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew York City\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__place__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJohn Smith\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Add more commonly occurring phrases as needed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the SpaCy English model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Perform NER using SpaCy\u001b[39;00m\n\u001b[1;32m     19\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text using NLTK\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Define commonly occurring phrases to be replaced\n",
    "    phrase_mapping = {\n",
    "        'New York City': '__place__',\n",
    "        'John Smith': '__name__',\n",
    "        # Add more commonly occurring phrases as needed\n",
    "    }\n",
    "    \n",
    "    # Load the SpaCy English model\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "    \n",
    "    # Perform NER using SpaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    preprocessed_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Replace commonly occurring phrases with a single word\n",
    "        if token in phrase_mapping:\n",
    "            preprocessed_tokens.append(phrase_mapping[token])\n",
    "        # Replace proper nouns with tags\n",
    "        else:\n",
    "            is_proper_noun = any(ent.text == token and ent.label_ in ['PERSON', 'GPE'] for ent in doc.ents)\n",
    "            if is_proper_noun:\n",
    "                if token.istitle():\n",
    "                    preprocessed_tokens.append('__name__')\n",
    "                else:\n",
    "                    preprocessed_tokens.append('__place__')\n",
    "            else:\n",
    "                preprocessed_tokens.append(token)\n",
    "    \n",
    "    # Join the preprocessed tokens back into a text\n",
    "    preprocessed_text = ' '.join(preprocessed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"John Smith is a software engineer from New York City. He works at XYZ Company.\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500325_2012_cleantext.txt', '500325_2016_cleantext.txt', '500325_2011_cleantext.txt', '500325_2015_cleantext.txt', '500325_2014_cleantext.txt', '500325_2010_cleantext.txt', '500325_2017_cleantext.txt', '500325_2013_cleantext.txt']\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "path = path = \"./Reliance_Dataset\"\n",
    "txts = [x for x in listdir(path) if x[-3:] == 'txt']\n",
    "print(txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chetanyabhan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Token  TF-IDF score\n",
      "0         investigate      0.000610\n",
      "1          committee1      0.000610\n",
      "2              powers      0.001220\n",
      "3      riskmanagement      0.000610\n",
      "4      theperformance      0.000610\n",
      "...               ...           ...\n",
      "10171  forwardlooking      0.007318\n",
      "10172          future      0.039031\n",
      "10173             new      0.107944\n",
      "10174           india      0.198203\n",
      "10175      partnering      0.012807\n",
      "\n",
      "[10176 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "corpus = ''\n",
    "for f in txts:\n",
    "    with open(\"Reliance_Dataset/\" + f, 'r') as file:\n",
    "        text = file.read()\n",
    "        corpus += '\\n' + text \n",
    "\n",
    "TIFDIF(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Model of the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('slowdown', 0.8093000650405884), ('East', 0.8064174056053162), ('Government', 0.8059264421463013), ('Asia', 0.8056067228317261), ('expected', 0.8050495386123657), ('need', 0.8050317764282227), ('globally', 0.8050126433372498), ('grow', 0.8047935366630554), ('cost', 0.804621696472168), ('Retail', 0.8043539524078369)]\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocess(corpus)\n",
    "model = Word2Vec([tokens], vector_size=300, window=5, min_count=1, epochs = 20)\n",
    "word_vectors = model.wv\n",
    "print(model.wv.most_similar('collaborate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('JCB', 0.23696570098400116), ('campaignis', 0.2305629700422287), ('disruptions', 0.20600491762161255), ('theBest', 0.20320232212543488), ('Circulation', 0.19682489335536957), ('Processing', 0.19563399255275726), ('succeedingmeeting', 0.1877049207687378), ('reinstallationof', 0.18250811100006104), ('premise', 0.17723533511161804), ('circle6Maximising', 0.17693930864334106)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('collaboration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Benzene', 0.6686267852783203), ('users', 0.6644120216369629), ('design', 0.6638323664665222), ('respectively', 0.6637520790100098), ('Infotel', 0.6636399626731873), ('complex', 0.6630818843841553), ('economies', 0.6627340912818909), ('earnings', 0.66253662109375), ('always', 0.662131130695343), ('rating', 0.6611295342445374)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('collaborative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('coal', 0.4279607832431793), ('footwear', 0.4197697937488556), ('certain', 0.417576402425766), ('information', 0.4171597361564636), ('IndustrialRelations', 0.4156395494937897), ('efficient', 0.4138558506965637), ('effect', 0.41383904218673706), ('contract', 0.4135437607765198), ('oils', 0.41193899512290955), ('TV18', 0.41181012988090515)]\n"
     ]
    }
   ],
   "source": [
    "#print(model.wv.most_similar('cooperate')) # KeyError: \"Key 'cooperate' not present\"\n",
    "print(model.wv.most_similar('cooperation')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Companymade', 0.20959284901618958), ('strategyimplementation', 0.20767642557621002), ('ofLondon', 0.19712801277637482), ('Relief', 0.18848437070846558), ('environmentfriendly', 0.18557246029376984), ('tocredit', 0.18368777632713318), ('innumerable', 0.1804037094116211), ('Squarethrough', 0.18008442223072052), ('VFA', 0.17502018809318542), ('Belt', 0.17446376383304596)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('teamwork'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
